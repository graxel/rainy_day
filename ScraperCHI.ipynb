{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago weather data for  10 / 2018  scraped in  166.474797 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import selenium\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def xpath(row=0,col=0,sub=0,header=False):\n",
    "    if header == True:\n",
    "        xp = '//app/city-history/city-history-layout/div/div[2]/'\\\n",
    "            + 'section/div[2]/div[3]/div/div/div/div/'\\\n",
    "            + 'city-history-observation/div/div[2]/table'\\\n",
    "            + '/thead/tr/td['+str(col+1)+']'\n",
    "    else:\n",
    "        xp = '//app/city-history/city-history-layout/div/div[2]/'\\\n",
    "            + 'section/div[2]/div[3]/div/div/div/div/'\\\n",
    "            + 'city-history-observation/div/div[2]/table'\\\n",
    "            + '/tbody/tr/td[' + str(col+1) + ']'\\\n",
    "            + '/table/tbody/tr[' + str(row+1) + ']'\\\n",
    "            + '/td[' + str(sub+1) + ']'\n",
    "    return xp\n",
    "\n",
    "\n",
    "\n",
    "def read_webpage(driver,url):\n",
    "    # Record start time (for scraper performance purposes)\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    webpage_data = []\n",
    "\n",
    "    row_idx = 0\n",
    "    column_idx = 0\n",
    "    row_data = []\n",
    "    while True:\n",
    "        # For each column,\n",
    "        # ...initialize subcolumn_idx to 0 and column_data to [], and ...\n",
    "        subcolumn_idx = 0\n",
    "        column_data = []\n",
    "        try:\n",
    "            # ...try to read the subcolumn at the following xpath to see if the column exists.\n",
    "            raw_main_header = driver.find_element_by_xpath(xpath(col=column_idx, header=True))\n",
    "            # If that worked, then iterate through subcolumns.\n",
    "            while True:\n",
    "                # For each subcolumn,\n",
    "                try:\n",
    "                    # ...try to read the subcolumn at the following xpath to see if it exists.\n",
    "                    raw_sub_header = driver.find_element_by_xpath(xpath(row=row_idx,col=column_idx,sub=subcolumn_idx))\n",
    "                    header = raw_sub_header.text+' '+raw_main_header.text\n",
    "                    # If that worked, then store the concatenation of sub_header and main_header in column_data.\n",
    "                    column_data.append(header)\n",
    "                except:\n",
    "                    # If it didn't work, there is no subcolumn at that index. Break subcolumn loop.\n",
    "                    break\n",
    "\n",
    "                # next subcolumn\n",
    "                subcolumn_idx += 1\n",
    "\n",
    "            # That's it for the subcolumns in this column. Store column_data in row_data.\n",
    "            row_data.extend(column_data)\n",
    "\n",
    "        except:\n",
    "            # If it didn't work, there is no column at that index. Break column loop.\n",
    "            break\n",
    "\n",
    "        # next column\n",
    "        column_idx += 1\n",
    "\n",
    "    # That's it for the columns in this row.  Store row_data in webpage_data.\n",
    "    webpage_data.append(row_data)\n",
    "    \n",
    "\n",
    "    # rows\n",
    "    row_idx = 1\n",
    "    column_idx = 0\n",
    "    subcolumn_idx = 0\n",
    "\n",
    "    # Iterate through rows.\n",
    "    while True:\n",
    "        # For each row,\n",
    "        # ...initialize column_idx to 0 and row_data to [], and ...\n",
    "        column_idx = 0\n",
    "        row_data = []\n",
    "                \n",
    "        try:\n",
    "            # ...try to read the subcolumn at the following xpath to see if the row exists.\n",
    "            raw_datum = driver.find_element_by_xpath(xpath(row=row_idx,col=column_idx,sub=subcolumn_idx))\n",
    "\n",
    "            # If that worked, then iterate through columns.\n",
    "            while True:\n",
    "                # For each column,\n",
    "                # ...initialize subcolumn_idx to 0 and column_data to [], and ...\n",
    "                subcolumn_idx = 0\n",
    "                column_data = []\n",
    "                \n",
    "                try:\n",
    "                    # ...try to read the subcolumn at the following xpath to see if the column exists.\n",
    "                    raw_datum = driver.find_element_by_xpath(xpath(row=row_idx,col=column_idx,sub=subcolumn_idx))\n",
    "\n",
    "                    # If that worked, then iterate through subcolumns\n",
    "                    while True:\n",
    "                        # For each subcolumn,\n",
    "                        try:\n",
    "                            # ...try to read the subcolumn at the following xpath to see if it exists.\n",
    "                            raw_datum = driver.find_element_by_xpath(xpath(row=row_idx,col=column_idx,sub=subcolumn_idx))\n",
    "\n",
    "                            # If that worked, then try to convert raw_datum.text to a float.\n",
    "                            try:\n",
    "                                if column_idx == 0:\n",
    "                                    datum = int(raw_datum.text) # Store dates as integers.\n",
    "                                else:\n",
    "                                    datum = float(raw_datum.text) # Store measured values as floats.\n",
    "                            except:\n",
    "                                datum = float('nan') # Store dashes and anything that doesn't cooperate as NaNs.\n",
    "                            column_data.append(datum)\n",
    "                        except:\n",
    "                            # If it didn't work, there is no subcolumn at that index. Break subcolumn loop.\n",
    "                            break\n",
    "\n",
    "                        # next subcolumn\n",
    "                        subcolumn_idx += 1\n",
    "\n",
    "                    # That's it for the subcolumns in this column. Store column_data in row_data.\n",
    "                    row_data.extend(column_data)\n",
    "\n",
    "                except:\n",
    "                    #If it didn't work, there is no column at that index. \n",
    "                    break\n",
    "\n",
    "                # next column\n",
    "                column_idx += 1\n",
    "\n",
    "            # That's it for the columns in this row.  Store row_data in webpage_data.\n",
    "            webpage_data.append(row_data)\n",
    "            \n",
    "        except:\n",
    "            # If it didn't work, there is no row at that index.\n",
    "            break\n",
    "\n",
    "        # next row\n",
    "        row_idx += 1\n",
    "\n",
    "    # That's it for the rows in the table. Done with this webpage!\n",
    "\n",
    "    stop_time = datetime.now()\n",
    "    time_diff = stop_time-start_time\n",
    "\n",
    "    return webpage_data, time_diff\n",
    "\n",
    "\n",
    "chromedriver = '/Users/grazillionaire/Documents/chromedriver' # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "\n",
    "# Generate list of dates (months) to pull weather data for.\n",
    "\n",
    "# For Chicago\n",
    "city = 'il/chicago/KORD'\n",
    "start_date = datetime(2001,1,1)\n",
    "end_date = datetime(2018,6,30)\n",
    "\n",
    "# For New York\n",
    "#city = 'ny/new-york/KNYC'\n",
    "#start_date = datetime(2010,5,5)\n",
    "#end_date = datetime.today()\n",
    "\n",
    "date_list = pd.Series(pd.date_range(start_date,end_date))\n",
    "date_list = [datetime.today()] ################# delete this ##################\n",
    "#city = 'ca/san-francisco/KSFO' ################ delete this ##################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for date in date_list:\n",
    "    \n",
    "    url = 'https://www.wunderground.com/history/monthly/us/{loc}/date/{y}-{m}?cm_ven=localwx_history'\\\n",
    "          .format(loc=city, y=date.year, m=date.month)\n",
    "    \n",
    "    webpage_data, elapsed = read_webpage(driver, url)\n",
    "\n",
    "    print('Chicago weather data for ',date.month,'/',date.year,' scraped in ', elapsed.seconds + elapsed.microseconds/1000000, 's')\n",
    "\n",
    "\n",
    "    myFile = open('CHI weather data/CHI-{y}-{m}.csv'.format(y=date.year, m=date.month), 'w')\n",
    "    with myFile:\n",
    "        writer = csv.writer(myFile)\n",
    "        writer.writerows(webpage_data)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oct Time', 'Max Temperature (° F)', 'Avg Temperature (° F)', 'Min Temperature (° F)', 'Max Dew Point (° F)', 'Avg Dew Point (° F)', 'Min Dew Point (° F)', 'Max Humidity (%)', 'Avg Humidity (%)', 'Min Humidity (%)', 'Max Wind Speed (mph)', 'Avg Wind Speed (mph)', 'Min Wind Speed (mph)', 'Max Pressure (Hg)', 'Avg Pressure (Hg)', 'Min Pressure (Hg)', 'Max Precipation (in)', 'Avg Precipation (in)', 'Min Precipation (in)']\n",
      "[1, 72.0, 65.0, 57.0, 64.0, 58.0, 54.0, 93.0, nan, 68.0, 18.0, nan, 0.0, 30.23, nan, 29.97, nan, 1.56, nan]\n",
      "[2, 67.0, 63.0, 58.0, 65.0, 57.0, 54.0, 93.0, nan, 72.0, 15.0, nan, 0.0, 30.11, nan, 29.97, nan, 0.01, nan]\n",
      "[3, 88.0, 74.0, 59.0, 68.0, 62.0, 56.0, 93.0, nan, 45.0, 28.0, nan, 6.0, 30.05, nan, 29.68, nan, 0.0, nan]\n",
      "[4, 77.0, 65.0, 52.0, 69.0, 51.0, 41.0, 93.0, nan, 61.0, 24.0, nan, 9.0, 30.22, nan, 29.75, nan, 0.04, nan]\n",
      "[5, 65.0, 60.0, 54.0, 63.0, 57.0, 45.0, 93.0, nan, 62.0, 16.0, nan, 0.0, 30.12, nan, 29.91, nan, 1.24, nan]\n",
      "[6, 70.0, 64.0, 57.0, 66.0, 59.0, 50.0, 93.0, nan, 77.0, 17.0, nan, 0.0, 30.15, nan, 29.87, nan, 0.12, nan]\n",
      "[7, 63.0, 60.0, 57.0, 61.0, 56.0, 51.0, 100.0, nan, 80.0, 17.0, nan, 0.0, 30.22, nan, 30.09, nan, 0.52, nan]\n",
      "[8, 85.0, 74.0, 63.0, 69.0, 64.0, 61.0, 100.0, nan, 48.0, 21.0, nan, 0.0, 30.12, nan, 29.99, nan, 0.15, nan]\n",
      "[9, 85.0, 78.0, 70.0, 68.0, 66.0, 63.0, 90.0, nan, 51.0, 21.0, nan, 5.0, 30.01, nan, 29.83, nan, 0.01, nan]\n",
      "[10, 76.0, 74.0, 73.0, 63.0, 63.0, 63.0, 71.0, nan, 55.0, 15.0, nan, 12.0, 29.81, nan, 29.79, nan, 0.0, nan]\n"
     ]
    }
   ],
   "source": [
    "for row in webpage_data:\n",
    "    print(row)\n",
    "    #print([webpage_data[0][0][:3]+' '+str(row[0])]+row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subcolumn_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//app/city-history/city-history-layout/div/div[2]/section/div[2]/div[3]/div/div/div/div/city-history-observation/div/div[2]/table/tbody/tr/td[5]/table/tbody/tr[4]/td[6]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xpath(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//app/city-history/city-history-layout/div/div[2]/section/div[2]/div[3]/div/div/div/div/city-history-observation/div/div[2]/table/tbody/tr/td[11]/table/tbody/tr[1]/td[1]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xpath(col=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
